{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['TAVILY_API_KEY']= os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [To-do] 웨딩 정보 관련 데이터 수집 -> 코드 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 로드 완료: 총 4개의 문서 청크 생성\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/f4f9x_vx7zg0vcl_gmq4gd0c0000gn/T/ipykernel_72171/789810345.py:118: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = self.qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "답변: 제공된 PDF에서 관련 정보를 찾을 수 없습니다. \n",
      "\n",
      "일반적인 지식을 바탕으로 답변드리자면, 예산 관리를 위해 다음과 같은 팁을 고려할 수 있습니다:\n",
      "\n",
      "1. **예산 설정**: 전체 예산을 설정하고, 각 항목(예: 드레스, 메이크업, 촬영, 음식 등)에 대한 예산을 분배합니다.\n",
      "\n",
      "2. **우선순위 정하기**: 가장 중요하게 생각하는 항목에 더 많은 예산을 할당하고, 덜 중요한 항목은 줄이는 방법을 고려합니다.\n",
      "\n",
      "3. **비교 쇼핑**: 여러 업체의 가격을 비교하고, 리뷰를 참고하여 가성비가 좋은 선택을 합니다.\n",
      "\n",
      "4. **비용 절감 방법**: DIY(Do It Yourself)로 할 수 있는 부분은 직접 하거나, 친구나 가족의 도움을 받는 것도 좋은 방법입니다.\n",
      "\n",
      "5. **예비비 마련**: 예상치 못한 비용이 발생할 수 있으므로, 전체 예산의 10% 정도는 예비비로 남겨두는 것이 좋습니다.\n",
      "\n",
      "6. **계약서 확인**: 계약서를 꼼꼼히 읽고, 숨겨진 비용이 없는지 확인합니다.\n",
      "\n",
      "7. **할인 및 프로모션 활용**: 결혼식 관련 업체에서 제공하는 할인이나 프로모션을 적극 활용합니다.\n",
      "\n",
      "이러한 팁들을 통해 예산을 효과적으로 관리할 수 있습니다.\n",
      "\n",
      "참조된 페이지:\n",
      "- Page Unknown\n",
      "- Page Unknown\n",
      "- Page Unknown\n",
      "상담을 종료합니다. 감사합니다!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class PDFBasedRAG:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.1,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200\n",
    "    ):\n",
    "      \n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        self.vector_store = None\n",
    "        self.qa_chain = None\n",
    "\n",
    "    def load_pdf(self, pdf_path: str) -> None:\n",
    "        \"\"\"\n",
    "        PDF 파일 로드 및 처리\n",
    "        \n",
    "        Args:\n",
    "            pdf_path: PDF 파일 경로\n",
    "        \"\"\"\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF 파일을 찾을 수 없습니다: {pdf_path}\")\n",
    "  \n",
    "        loader = TextLoader(pdf_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "\n",
    "        split_documents = self.text_splitter.split_documents(documents)\n",
    "        \n",
    "        # 벡터 저장소 생성\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=split_documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        # QA 체인 설정\n",
    "        self._setup_qa_chain()\n",
    "        \n",
    "        print(f\"PDF 로드 완료: 총 {len(split_documents)}개의 문서 청크 생성\")\n",
    "\n",
    "\n",
    "    def _setup_qa_chain(self) -> None:\n",
    "        \"\"\"QA 체인 설정\"\"\"\n",
    "        # 프롬프트 템플릿 \n",
    "        prompt_template = \"\"\"주어진 컨텍스트를 기반으로 질문에 답변해주세요.\n",
    "        \n",
    "        컨텍스트에서 답을 찾을 수 없는 경우:\n",
    "        1) \"제공된 PDF에서 관련 정보를 찾을 수 없습니다.\"라고 명시하고\n",
    "        2) 일반적인 지식을 바탕으로 답변해주세요.\n",
    "\n",
    "        컨텍스트:\n",
    "        {context}\n",
    "\n",
    "        질문: {question}\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "        # QA 체인 생성\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.vector_store.as_retriever(\n",
    "                search_kwargs={\"k\": 3}\n",
    "            ),\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": PROMPT}\n",
    "        )\n",
    "\n",
    "    def answer_query(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        질문에 대한 답변 생성\n",
    "        \n",
    "        Args:\n",
    "            query: 질문 내용\n",
    "            \n",
    "        Returns:\n",
    "            답변과 참조 문서 정보를 포함한 딕셔너리\n",
    "        \"\"\"\n",
    "\n",
    "        if self.vector_store is None:\n",
    "            # PDF가 로드되지 않은 경우 LLM만 사용\n",
    "            response = self.llm.predict(query)\n",
    "            return {\n",
    "                \"answer\": \"PDF가 로드되지 않아 일반적인 답변만 제공됩니다:\\n\" + response,\n",
    "                \"source_documents\": []\n",
    "            }\n",
    "\n",
    "        # RAG를 사용한 답변 생성\n",
    "        response = self.qa_chain({\"query\": query})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response[\"result\"],\n",
    "            \"source_documents\": [\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"page\": doc.metadata.get(\"page\", \"Unknown\")\n",
    "                }\n",
    "                for doc in response[\"source_documents\"]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "def main():\n",
    "   \n",
    "    try:\n",
    "        rag = PDFBasedRAG(\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        pdf_path = \"wedding_info_data.txt\"  #  파일 경로\n",
    "        rag.load_pdf(pdf_path)\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"\\n질문: \")\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"상담을 종료합니다. 감사합니다!\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                response = rag.answer_query(user_input)\n",
    "                print(\"\\n답변:\", response[\"answer\"])\n",
    "                print(\"\\n참조된 페이지:\")\n",
    "                for source in response[\"source_documents\"]:\n",
    "                    print(f\"- Page {source['page']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\n오류가 발생했습니다: {str(e)}\")\n",
    "        \n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"에러 발생: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wedding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
